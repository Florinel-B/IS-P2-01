"""
Ensemble Model: LSTM Anomal√≠as + Random Forest Cuelgues
Predice 3 clases: 0=Normal, 1=Anomal√≠a Voltaje (+0.5V), 2=Cuelgue Sistema (2 min)
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import pickle
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from typing import Tuple, Dict, List, Optional
from training_template import (
    VoltageDropDataset,
    importar_modelo_portable,
    DEVICE
)


class EnsembleAnomalyDetector:
    """
    Detector h√≠brido que combina:
    - LSTM para anomal√≠as de voltaje
    - Detecci√≥n de cuelgues basada en status
    - Random Forest para fusi√≥n y predicci√≥n multiclase
    """

    def __init__(self, lstm_model_path: str, rf_model_path: Optional[str] = None):
        """
        Args:
            lstm_model_path: Path al modelo LSTM (modelo_anomalias.pth o finetuned)
            rf_model_path: Path al Random Forest (opcional, se entrena si no existe)
        """
        self.lstm_model_path = lstm_model_path
        self.rf_model_path = rf_model_path or "modelo_ensemble_rf.pkl"

        # Cargar modelo LSTM
        print(f"üìä Cargando modelo LSTM desde {lstm_model_path}...")
        self.lstm_dict = importar_modelo_portable(lstm_model_path, device=DEVICE)
        self.lstm_model = self.lstm_dict["model"].to(DEVICE)
        self.lstm_threshold = self.lstm_dict.get("threshold", 0.5)
        self.scaler = self.lstm_dict["scaler"]
        self.seq_len = self.lstm_dict.get("seq_len", 60)

        # Random Forest (ser√° cargado o entrenado)
        self.rf_model = None
        self.rf_scaler = None  # Para normalizar features del ensemble

        print(f"   ‚úì LSTM threshold: {self.lstm_threshold:.2f}")
        print(f"   ‚úì Sequence length: {self.seq_len}")

    def detect_hangs(self, df: pd.DataFrame, hang_duration_minutes: int = 2) -> np.ndarray:
        """
        Detecta cuelgues del sistema (status=0 durante N minutos consecutivos).

        Args:
            df: DataFrame con columnas ['tiempo', 'status', ...]
            hang_duration_minutes: Duraci√≥n m√≠nima para considerar cuelgue

        Returns:
            Array binario [0=normal, 1=cuelgue] para cada timestamp
        """
        hang_labels = np.zeros(len(df), dtype=int)

        # Resetear index para garantizar alineaci√≥n
        df = df.reset_index(drop=True).copy()

        # Asegurar columna tiempo para c√°lculo de duraci√≥n
        if "tiempo" not in df.columns:
            df["tiempo"] = pd.date_range(end=pd.Timestamp.utcnow(), periods=len(df), freq="T")

        # Detectar periodos de cuelgue
        current_hang_start_idx = None
        current_hang_start_time = None

        for i, row in df.iterrows():
            status = row["status"]

            if status != 1:  # Sistema no est√° "1" (normal)
                if current_hang_start_idx is None:
                    current_hang_start_idx = i
                    current_hang_start_time = row["tiempo"]
            else:
                if current_hang_start_idx is not None:
                    duration = (row["tiempo"] - current_hang_start_time).total_seconds() / 60
                    if duration >= hang_duration_minutes:
                        # Marcar los timestamps del cuelgue (entre current_hang_start_idx e i)
                        for j in range(current_hang_start_idx, i):
                            if df.iloc[j]["status"] != 1:
                                hang_labels[j] = 1
                    current_hang_start_idx = None
                    current_hang_start_time = None

        # Verificar si el √∫ltimo periodo es un cuelgue
        if current_hang_start_idx is not None:
            duration = (df.iloc[-1]["tiempo"] - current_hang_start_time).total_seconds() / 60
            if duration >= hang_duration_minutes:
                for j in range(current_hang_start_idx, len(df)):
                    if df.iloc[j]["status"] != 1:
                        hang_labels[j] = 1

        return hang_labels

    def extract_lstm_features(self, df: pd.DataFrame, limit: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        Extrae predicciones del LSTM como features.

        Returns:
            (probabilities, predictions): Arrays de probabilidades y predicciones binarias
        """
        subset = df if limit is None else df.iloc[-limit:]
        dataset = VoltageDropDataset(subset, seq_len=self.seq_len, scaler=self.scaler)

        from torch.utils.data import DataLoader

        loader = DataLoader(dataset, batch_size=32, shuffle=False, pin_memory=torch.cuda.is_available(), num_workers=0)

        self.lstm_model.eval()
        all_probs = []
        all_preds = []

        with torch.no_grad():
            for X_batch, _ in loader:
                X_batch = X_batch.to(DEVICE)
                probs, _ = self.lstm_model.predict(X_batch)
                preds = (probs >= self.lstm_threshold).float()

                all_probs.extend(probs.cpu().numpy().flatten())
                all_preds.extend(preds.cpu().numpy().flatten())

        return np.array(all_probs), np.array(all_preds)

    def create_ensemble_features(
        self,
        df: pd.DataFrame,
        lstm_probs: np.ndarray,
        hang_labels: np.ndarray
    ) -> np.ndarray:
        """
        Crea matrix de features para el Random Forest combinando:
        - Predicciones LSTM (probabilidad, predicci√≥n)
        - Labels de cuelgues
        - Features estad√≠sticas de voltaje
        - Status
        """
        n_samples = len(df)
        
        # Verificar que todos tienen el mismo tama√±o
        assert len(lstm_probs) == n_samples, f"lstm_probs size mismatch: {len(lstm_probs)} vs {n_samples}"
        assert len(hang_labels) == n_samples, f"hang_labels size mismatch: {len(hang_labels)} vs {n_samples}"
        
        features_list = []

        # 1. LSTM probability and prediction
        features_list.append(lstm_probs.reshape(-1, 1))
        features_list.append((lstm_probs >= self.lstm_threshold).astype(float).reshape(-1, 1))

        # 2. Hang detection
        features_list.append(hang_labels.astype(float).reshape(-1, 1))

        # 3. Status column (si existe)
        if "status" in df.columns:
            features_list.append(df["status"].values.astype(float).reshape(-1, 1))

        X = np.hstack(features_list)
        return X

    def train_random_forest(
        self,
        df: pd.DataFrame,
        lstm_probs: np.ndarray,
        hang_labels: np.ndarray,
        target_labels: np.ndarray,
        n_estimators: int = 100,
        max_depth: int = 15
    ) -> None:
        """
        Entrena el Random Forest con los features del ensemble.

        Args:
            target_labels: Array con valores 0 (normal), 1 (anomal√≠a voltaje), 2 (cuelgue)
        """
        print("\nüå≥ Entrenando Random Forest Ensemble...")

        # Ajustar tama√±os: LSTM produce seq_len - 1 muestras menos
        n_df = len(df)
        n_lstm = len(lstm_probs)
        offset = n_df - n_lstm

        if offset > 0:
            # Padding al inicio para alinear
            lstm_probs = np.concatenate([np.zeros(offset), lstm_probs])
            # hang_labels ya tiene el tama√±o correcto, solo reordenar
            hang_labels = hang_labels[-n_df:]  # Tomar √∫ltimas n_df

        # Crear features
        X = self.create_ensemble_features(df, lstm_probs, hang_labels)

        # Normalizar features
        self.rf_scaler = StandardScaler()
        X_scaled = self.rf_scaler.fit_transform(X)

        # Entrenar RF
        self.rf_model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=42,
            n_jobs=-1,
            class_weight="balanced"  # Manejar desbalance de clases
        )

        self.rf_model.fit(X_scaled, target_labels)

        # Guardar modelo
        joblib.dump(self.rf_model, self.rf_model_path)
        joblib.dump(self.rf_scaler, self.rf_model_path.replace(".pkl", "_scaler.pkl"))

        print(f"   ‚úì Modelo guardado: {self.rf_model_path}")

        # Importancia de features
        feature_names = [
            "LSTM_prob",
            "LSTM_pred",
            "Hang_label",
            "Status"
        ]
        print("\n   Feature importance:")
        for name, importance in zip(feature_names, self.rf_model.feature_importances_):
            print(f"      {name:20s}: {importance:.4f}")

    def load_random_forest(self) -> bool:
        """Carga el Random Forest entrenado."""
        try:
            self.rf_model = joblib.load(self.rf_model_path)
            scaler_path = self.rf_model_path.replace(".pkl", "_scaler.pkl")
            self.rf_scaler = joblib.load(scaler_path)
            print(f"‚úì Random Forest cargado desde {self.rf_model_path}")
            return True
        except FileNotFoundError:
            print(f"‚ö†Ô∏è  No se encontr√≥ Random Forest. Ser√° necesario entrenarlo primero.")
            return False

    def predict(
        self,
        df: pd.DataFrame,
        use_lstm_only: bool = False
    ) -> Dict[str, np.ndarray]:
        """
        Realiza predicci√≥n multiclase (0/1/2).
        Funciona incluso con DataFrames peque√±os (< seq_len).

        Args:
            df: DataFrame con datos
            use_lstm_only: Si True, solo usa LSTM (sin RF)

        Returns:
            Dict con:
                - 'predictions': Array de predicciones (0/1/2)
                - 'probabilities': Matriz de probabilidades
                - 'lstm_probs': Probabilidades del LSTM
                - 'hang_labels': Labels de cuelgues
        """
        # Extraer features LSTM
        lstm_probs, lstm_preds = self.extract_lstm_features(df)
        print(f"   ‚úì LSTM: {lstm_preds.sum():.0f} anomal√≠as detectadas")

        # Detectar cuelgues
        hang_labels = self.detect_hangs(df)
        print(f"   ‚úì Cuelgues: {hang_labels.sum():.0f} periodos detectados")

        # Ajustar tama√±os: LSTM produce seq_len - 1 muestras menos
        # Padding: repetir √∫ltimo valor para alinear
        n_df = len(df)
        n_lstm = len(lstm_probs)
        offset = n_df - n_lstm

        if offset > 0:
            # Padding al inicio para alinear
            lstm_probs_padded = np.concatenate([np.zeros(offset), lstm_probs])
            lstm_preds_padded = np.concatenate([np.zeros(offset), lstm_preds])
            lstm_probs = lstm_probs_padded
            lstm_preds = lstm_preds_padded

        # Si no hay RF, combinar heur√≠sticamente
        if use_lstm_only or self.rf_model is None:
            print("   ‚ö†Ô∏è  Usando l√≥gica heur√≠stica (sin RF)")
            predictions = np.zeros(len(df), dtype=int)

            for i in range(len(df)):
                if hang_labels[i] == 1:
                    predictions[i] = 2  # Cuelgue
                elif lstm_preds[i] == 1:
                    predictions[i] = 1  # Anomal√≠a voltaje
                # else: 0 (normal, default)

            probabilities = np.zeros((len(df), 3))
            for i in range(len(df)):
                probabilities[i, predictions[i]] = 1.0

            return {
                "predictions": predictions,
                "probabilities": probabilities,
                "lstm_probs": lstm_probs,
                "hang_labels": hang_labels,
                "method": "heuristic"
            }

        # Usar Random Forest
        print("   ‚úì Usando Random Forest Ensemble")
        X = self.create_ensemble_features(df, lstm_probs, hang_labels)
        X_scaled = self.rf_scaler.transform(X)

        predictions = self.rf_model.predict(X_scaled)
        probabilities_rf = self.rf_model.predict_proba(X_scaled)

        # Asegurar que probabilities tenga 3 columnas (0, 1, 2)
        # El RF puede predecir solo 2 clases si no hay ejemplos de la 3¬™
        if probabilities_rf.shape[1] < 3:
            probabilities = np.zeros((len(df), 3))
            for i, class_id in enumerate(self.rf_model.classes_):
                probabilities[:, class_id] = probabilities_rf[:, i]
        else:
            probabilities = probabilities_rf

        print(f"   Resultados: {np.sum(predictions == 0)} normal, "
              f"{np.sum(predictions == 1)} anomal√≠a voltaje, "
              f"{np.sum(predictions == 2)} cuelgues")

        return {
            "predictions": predictions,
            "probabilities": probabilities,
            "lstm_probs": lstm_probs,
            "hang_labels": hang_labels,
            "method": "random_forest"
        }

    def get_classification_names(self, predictions: np.ndarray) -> List[str]:
        """Convierte predicciones num√©ricas a nombres legibles."""
        class_names = {
            0: "Normal",
            1: "Anomal√≠a Voltaje (+0.5V)",
            2: "Cuelgue Sistema"
        }
        return [class_names.get(int(p), "Unknown") for p in predictions]


def create_target_labels(df: pd.DataFrame, hang_duration_minutes: int = 2) -> np.ndarray:
    """
    Crea labels de verdad fundamental para entrenamiento del RF.
    Combina:
    - Anomal√≠a voltaje: aumentos >0.5V
    - Cuelgues: status != 1 durante N minutos
    """
    target = np.zeros(len(df), dtype=int)

    # Detectar cuelgues
    detector = EnsembleAnomalyDetector("modelo_anomalias.pth")  # Dummy instance
    hang_labels = detector.detect_hangs(df, hang_duration_minutes)

    # Detectar anomal√≠as de voltaje (cambios grandes)
    voltage_cols = [col for col in df.columns if "voltage" in col.lower()]
    if voltage_cols:
        for i in range(1, len(df)):
            prev_voltages = df.iloc[i - 1][voltage_cols].values
            curr_voltages = df.iloc[i][voltage_cols].values

            voltage_diffs = np.abs(curr_voltages - prev_voltages)
            if np.any(voltage_diffs > 0.5):
                target[i] = 1  # Anomal√≠a voltaje

    # Cuelgues sobrescriben (prioridad)
    target[hang_labels == 1] = 2

    return target
